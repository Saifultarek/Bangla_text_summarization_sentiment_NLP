{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_sfiH0btsdd"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "_mKijJ5svHV_",
        "outputId": "1159fda4-34a0-4ab8-99e1-9018f2ac78cd"
      },
      "source": [
        "df=pd.read_excel(\"/content/data.xlsx\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                Timestamp                                               Text  \\\n",
              "0 2022-03-21 00:19:53.236  প্রথমে ব্যাটিং–ব্যর্থতা, এরপর বোলারদের অসহায়ত্...   \n",
              "1 2022-03-21 01:11:34.998                                             অহিংস    \n",
              "2 2022-03-21 01:11:49.878  রতন হাত ধরলাে মালার-দেখেছাে, বলিনি আজ জ্যোৎস্ন...   \n",
              "3 2022-03-21 01:15:41.769  বর্তমান সরকারের ভিশন হচ্ছে ২০২১ সালের মধ্যে ক্...   \n",
              "4 2022-03-21 01:20:34.992  ডিজিটাল বাংলাদেশ একটি প্রত্যয়, যা বাংলাদেশের ...   \n",
              "\n",
              "                                             Summary  \n",
              "0  ব্যাটিং–ব্যর্থতা, এরপর বোলারদের অসহায়ত্ব হতাশা...  \n",
              "1  আমি মুক্ত স্নায়ুর, অবিরাম রক্তের স্রোত\\nআমি আম...  \n",
              "2        বলিনি আজ জ্যোৎস্না, চলাে-আজ পাগল হয়ে যাবাে  \n",
              "3  বর্তমান সরকার নারীর ক্ষমতায়ন নিশ্চিত করতে প্র...  \n",
              "4  ডিজিটাল বাংলাদেশ বর্তমান সরকারের এক সফল উন্নয়ন...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a911c433-9078-4e30-a175-329b6cb3bf4f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Timestamp</th>\n",
              "      <th>Text</th>\n",
              "      <th>Summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2022-03-21 00:19:53.236</td>\n",
              "      <td>প্রথমে ব্যাটিং–ব্যর্থতা, এরপর বোলারদের অসহায়ত্...</td>\n",
              "      <td>ব্যাটিং–ব্যর্থতা, এরপর বোলারদের অসহায়ত্ব হতাশা...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2022-03-21 01:11:34.998</td>\n",
              "      <td>অহিংস</td>\n",
              "      <td>আমি মুক্ত স্নায়ুর, অবিরাম রক্তের স্রোত\\nআমি আম...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2022-03-21 01:11:49.878</td>\n",
              "      <td>রতন হাত ধরলাে মালার-দেখেছাে, বলিনি আজ জ্যোৎস্ন...</td>\n",
              "      <td>বলিনি আজ জ্যোৎস্না, চলাে-আজ পাগল হয়ে যাবাে</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2022-03-21 01:15:41.769</td>\n",
              "      <td>বর্তমান সরকারের ভিশন হচ্ছে ২০২১ সালের মধ্যে ক্...</td>\n",
              "      <td>বর্তমান সরকার নারীর ক্ষমতায়ন নিশ্চিত করতে প্র...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2022-03-21 01:20:34.992</td>\n",
              "      <td>ডিজিটাল বাংলাদেশ একটি প্রত্যয়, যা বাংলাদেশের ...</td>\n",
              "      <td>ডিজিটাল বাংলাদেশ বর্তমান সরকারের এক সফল উন্নয়ন...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a911c433-9078-4e30-a175-329b6cb3bf4f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a911c433-9078-4e30-a175-329b6cb3bf4f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a911c433-9078-4e30-a175-329b6cb3bf4f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(\"Timestamp\",axis=1)\n",
        "df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "mCj8E6S8CK1Z",
        "outputId": "bad3ee26-e97a-4776-e937-8a92961b89e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                Text  \\\n",
              "0  প্রথমে ব্যাটিং–ব্যর্থতা, এরপর বোলারদের অসহায়ত্...   \n",
              "1                                             অহিংস    \n",
              "2  রতন হাত ধরলাে মালার-দেখেছাে, বলিনি আজ জ্যোৎস্ন...   \n",
              "3  বর্তমান সরকারের ভিশন হচ্ছে ২০২১ সালের মধ্যে ক্...   \n",
              "4  ডিজিটাল বাংলাদেশ একটি প্রত্যয়, যা বাংলাদেশের ...   \n",
              "\n",
              "                                             Summary  \n",
              "0  ব্যাটিং–ব্যর্থতা, এরপর বোলারদের অসহায়ত্ব হতাশা...  \n",
              "1  আমি মুক্ত স্নায়ুর, অবিরাম রক্তের স্রোত\\nআমি আম...  \n",
              "2        বলিনি আজ জ্যোৎস্না, চলাে-আজ পাগল হয়ে যাবাে  \n",
              "3  বর্তমান সরকার নারীর ক্ষমতায়ন নিশ্চিত করতে প্র...  \n",
              "4  ডিজিটাল বাংলাদেশ বর্তমান সরকারের এক সফল উন্নয়ন...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c6e5c935-729e-4952-a536-057dfd3e73d7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>প্রথমে ব্যাটিং–ব্যর্থতা, এরপর বোলারদের অসহায়ত্...</td>\n",
              "      <td>ব্যাটিং–ব্যর্থতা, এরপর বোলারদের অসহায়ত্ব হতাশা...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>অহিংস</td>\n",
              "      <td>আমি মুক্ত স্নায়ুর, অবিরাম রক্তের স্রোত\\nআমি আম...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>রতন হাত ধরলাে মালার-দেখেছাে, বলিনি আজ জ্যোৎস্ন...</td>\n",
              "      <td>বলিনি আজ জ্যোৎস্না, চলাে-আজ পাগল হয়ে যাবাে</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>বর্তমান সরকারের ভিশন হচ্ছে ২০২১ সালের মধ্যে ক্...</td>\n",
              "      <td>বর্তমান সরকার নারীর ক্ষমতায়ন নিশ্চিত করতে প্র...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ডিজিটাল বাংলাদেশ একটি প্রত্যয়, যা বাংলাদেশের ...</td>\n",
              "      <td>ডিজিটাল বাংলাদেশ বর্তমান সরকারের এক সফল উন্নয়ন...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c6e5c935-729e-4952-a536-057dfd3e73d7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c6e5c935-729e-4952-a536-057dfd3e73d7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c6e5c935-729e-4952-a536-057dfd3e73d7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcjwgSiNEhKS",
        "outputId": "7464a99f-8789-475a-bf3c-d6964acfa95c"
      },
      "source": [
        "for i in range(5):\n",
        "    print(\"News:\",i+1)\n",
        "    print(\"Text:\",df.Text[i])\n",
        "    print(\"Summary:\",df.Summary[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "News: 1\n",
            "Text: প্রথমে ব্যাটিং–ব্যর্থতা, এরপর বোলারদের অসহায়ত্ব—ওয়ান্ডারার্সে দক্ষিণ আফ্রিকার বিপক্ষে দ্বিতীয় ওয়ানডে দেখতে আসা প্রবাসী বাংলাদেশিরা বাংলাদেশ দলের এই হতাশার ছবিটাই দেখে গেছেন। প্রথম ম্যাচে দক্ষিণ আফ্রিকাকে স্বচ্ছন্দে হারিয়ে ইতিহাস গড়া দলটির সঙ্গে আজকের বাংলাদেশের কোনো মিল নেই।\n",
            "Summary: ব্যাটিং–ব্যর্থতা, এরপর বোলারদের অসহায়ত্ব হতাশার ছবিটাই দেখে গেছেন\n",
            "News: 2\n",
            "Text: অহিংস \n",
            "Summary: আমি মুক্ত স্নায়ুর, অবিরাম রক্তের স্রোত\n",
            "আমি আমার মন যুদ্ধের সিংহাসনচ্যুত শাসক \n",
            "পুরোনো ধ্বংসের শক্তিরূপী শেষের শুরু হোক\n",
            "যা কিনা অহিংসার প্রাপক\n",
            "News: 3\n",
            "Text: রতন হাত ধরলাে মালার-দেখেছাে, বলিনি আজ জ্যোৎস্না, চলাে-আজ পাগল হয়ে যাবাে। \n",
            "মৃদু হাসে মালা 'বাকি নেই...কিন্তু একটা ব্যাপার, ধরাে যদি জীন বা পরী ভর করে? 'তবে ব্যাপারটা আগে থেকেই ঠিক করে নিচ্ছ, তুমি পরি আমি জীন রতন বললাে, বলে তারা এগােলাে। মালার মৃদু হাসির শব্দই বেশ বেজে ছিলাে, তারা টের পায়নি। \n",
            "তপনের ঘর পেরােনাের আগে সে ঘরের ভেতর থেকে তপন বলে,  'কে যায়?' একটু চমকেছিলাে বটে রতন, তবে মুহূর্তের মধ্যে সে সামলে নিলাে,চুপ শালা, আমি যাই। তুই চুড়ি পরা ধরলি কবে?\n",
            "Summary: বলিনি আজ জ্যোৎস্না, চলাে-আজ পাগল হয়ে যাবাে\n",
            "News: 4\n",
            "Text: বর্তমান সরকারের ভিশন হচ্ছে ২০২১ সালের মধ্যে ক্ষুধা ও দারিদ্র্যমুক্ত মধ্যম আয়ের বাংলাদেশ গড়ার লক্ষ্যে প্রধানমন্ত্রী শেখ হাসিনার বিশেষ উদ্যোগগুলোকে সর্বোচ্চ অগ্রাধিকার দিয়ে বাস্তবায়িত করা। এ প্রকল্পের ভিশন হচ্ছে, সব ক্ষেত্রে বৈষম্য দূর করে নারীর অংশগ্রহণের সুযোগ সৃষ্টি করা এবং মিশন হচ্ছে, পারিবারিক ও সামাজিক সিদ্ধান্ত গ্রহণে নারীর সম্পৃক্ততা বৃদ্ধি করা; শিক্ষা ও কর্মে নারীর অংশগ্রহণের সুযোগ সৃষ্টি করা; নারীর প্রতি বৈষম্য ও সহিংসতা প্রতিরোধ করা; নারীর সার্বিক মর্যাদা ও ক্ষমতায়নের লক্ষ্যে আইন, বিধি প্রণয়ন ও প্রতিষ্ঠা স্থাপন করা।\n",
            "Summary: বর্তমান সরকার নারীর ক্ষমতায়ন নিশ্চিত করতে প্রচেষ্টা চালিয়ে যাচ্ছে।\n",
            "News: 5\n",
            "Text: ডিজিটাল বাংলাদেশ একটি প্রত্যয়, যা বাংলাদেশের সাম্প্রতিক সময়ে অন্যতম একটি আলোচিত বিষয়। এর মূল লক্ষ্য, একুশ শতকে বাংলাদেশকে একটি তথ্য প্রযুক্তি নির্ভর রাষ্ট্র হিসেবে গড়ে তোলা এবং ২০২১ সালে বাংলাদেশের স্বাধীনতার সুবর্ণজয়ন্তী পালনের বছরে বাংলাদেশকে একটি মধ্যম আয়ের দেশে রূপান্তর করা।যখন এই ধারণাটি সামনে আসে, তখন সারা বিশ্বে তথ্য ও যোগাযোগ প্রযুক্তি এক বিপ্লবের সূচনা করেছিল। কিন্তু তখনও বাংলাদেশ তথ্য ও যোগাযোগ প্রযুক্তি ক্ষেত্রে অন্যান্য দেশের চেয়ে পিছিয়ে ছিল। তাই এই পরিকল্পনাটির মূল লক্ষ্য ছিল, একটি উন্নত দেশ, সমৃদ্ধ ডিজিটাল সমাজ, ডিজিটাল যুগের জনগোষ্ঠী, রূপান্তরিত উৎপাদনব্যবস্থা, নতুন জ্ঞানভিত্তিক অর্থনীতি - সর্বোপরি একটি জ্ঞান ও প্রযুক্তিভিত্তিক দেশ গঠন করা।\n",
            "Summary: ডিজিটাল বাংলাদেশ বর্তমান সরকারের এক সফল উন্নয়ন দর্শন।\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sPupDg-EpCS"
      },
      "source": [
        "contractions = { \n",
        "\"বি.দ্র \": \"বিশেষ দ্রষ্টব্য\",\n",
        "\"ড.\": \"ডক্টর\",\n",
        "\"ডা.\": \"ডাক্তার\",\n",
        "\"ইঞ্জি:\": \"ইঞ্জিনিয়ার\",\n",
        "\"রেজি:\": \"রেজিস্ট্রেশন\",\n",
        "\"মি.\": \"মিস্টার\",\n",
        "\"মু.\": \"মুহাম্মদ\",\n",
        "\"মো.\": \"মোহাম্মদ\",\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0ItqqpTEr5Z"
      },
      "source": [
        "import re\n",
        "import string\n",
        "def clean_text(text,remove_stopwords = False):\n",
        "    if True:\n",
        "        text = text.split()\n",
        "        new_text = []\n",
        "        for word in text:\n",
        "            if word in contractions:\n",
        "                new_text.append(contractions[word])\n",
        "            else:\n",
        "                new_text.append(word)\n",
        "        text = \" \".join(new_text)\n",
        "    # Format words and remove unwanted characters\n",
        "    whitespace = re.compile(u\"[\\s\\u0020\\u00a0\\u1680\\u180e\\u202f\\u205f\\u3000\\u2000-\\u200a]+\", re.UNICODE)\n",
        "    bangla_digits = u\"[\\u09E6\\u09E7\\u09E8\\u09E9\\u09EA\\u09EB\\u09EC\\u09ED\\u09EE\\u09EF]+\"\n",
        "    english_chars = u\"[a-zA-Z0-9]\"\n",
        "    punc = u\"[(),$%^&*+={}\\[\\]:\\\"|\\'\\~`<>/,¦!?½£¶¼©⅐⅑⅒⅓⅔⅕⅖⅗⅘⅙⅚⅛⅜⅝⅞⅟↉¤¿º;-]+\"\n",
        "    bangla_fullstop = u\"\\u0964\"     #bangla fullstop(dari)\n",
        "    punctSeq   = u\"['\\\"“”‘’]+|[.?!,…]+|[:;]+\"\n",
        "    \n",
        "    text = re.sub(bangla_digits, \" \", text)\n",
        "    text = re.sub(punc, \" \", text)\n",
        "    text = re.sub(english_chars, \" \", text)\n",
        "    text = re.sub(bangla_fullstop, \" \", text)\n",
        "    text = re.sub(punctSeq, \" \", text)\n",
        "    text = whitespace.sub(\" \", text).strip()\n",
        "    \n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\<a href', ' ', text)\n",
        "    text = re.sub(r'&amp;‘:‘ ’', '', text) \n",
        "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]। ,', ' ', text)\n",
        "    text = re.sub(r'<br />', ' ', text)\n",
        "    text = re.sub(r'\\'', ' ', text)\n",
        "    text = re.sub(r\"[\\@$#%~+-\\.\\'।\\\"]\",\" \",text)\n",
        "    text = re.sub(r\"(?m)^\\s+\", \"\", text)\n",
        "    text = re.sub(\"[()]\",\"\",text)\n",
        "    text = re.sub(\"[‘’]\",\"\",text)\n",
        "    text = re.sub(\"[!]\",\"\",text)\n",
        "    text = re.sub(\"[/]\",\"\",text)\n",
        "    text = re.sub(\"[:]\",\"\",text)\n",
        "    text= re.sub('\\ |\\?|\\.|\\!|\\/|\\;|\\:', ' ',text)\n",
        "    text= text.strip(\"/\")\n",
        "    \n",
        "    if remove_stopwords:\n",
        "        k = []\n",
        "        with open('/content/Banglastopword.txt', 'r',encoding=\"utf-8\") as f:\n",
        "            for word in f:\n",
        "                word = word.split()\n",
        "                k.append(word[0])\n",
        "            text = [t for t in text if t not in k]\n",
        "            text = \"\".join(text)\n",
        "            \n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJELFSYBE44z",
        "outputId": "d0c230dc-60dd-4d52-ea3e-6b11a91695e6"
      },
      "source": [
        "clean_summaries = []\n",
        "for summary in df.Summary:\n",
        "    clean_summaries.append(clean_text(summary,remove_stopwords=False))\n",
        "print(\"Summaries are complete.\")\n",
        "\n",
        "clean_texts = []\n",
        "for text in df.Text:\n",
        "    clean_texts.append(clean_text(text))\n",
        "print(\"Texts are complete.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summaries are complete.\n",
            "Texts are complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crg2xLmlE72Z"
      },
      "source": [
        "def count_words(count_dict, text):\n",
        "    for sentence in text:\n",
        "        for word in sentence.split():\n",
        "            if word not in count_dict:\n",
        "                count_dict[word] = 1\n",
        "            else:\n",
        "                count_dict[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwcYWsTOFCXB",
        "outputId": "35566e69-d04e-4633-eda7-06ede25a27e1"
      },
      "source": [
        "word_counts = {}\n",
        "count_words(word_counts, clean_summaries)\n",
        "count_words(word_counts, clean_texts)            \n",
        "print(\"Size of Vocabulary:\", len(word_counts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of Vocabulary: 6444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wg0-5TbPFFEh",
        "outputId": "cc103899-7905-43cc-d246-328e75fb4d30"
      },
      "source": [
        "word_counts[\"খারাপ\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4CXCy_RFJjB",
        "outputId": "c3c80966-2f59-49a4-ee02-2b29a8cca035"
      },
      "source": [
        "import numpy as np\n",
        "embeddings_index = {}\n",
        "with open('/content/bn_w2v_model.text', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split(' ')\n",
        "        word = values[0]\n",
        "        embedding = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = embedding\n",
        "\n",
        "print('Word embeddings:', len(embeddings_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word embeddings: 98164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZefSH5BFMN1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f402e8d-75e8-4024-b3c3-7924dd6409a4"
      },
      "source": [
        "embeddings_index[\"মানুষ\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-9.83040e-02, -1.39250e-02, -4.88280e-02,  8.12670e-02,\n",
              "       -7.05800e-02,  1.04904e-01, -1.24000e-04, -9.22190e-02,\n",
              "       -1.28943e-01,  4.79420e-02,  1.19970e-01,  8.07400e-03,\n",
              "       -2.50881e-01, -2.72280e-02, -1.05899e-01, -2.02090e-02,\n",
              "        2.05392e-01,  8.79960e-02, -1.55620e-02,  2.92981e-01,\n",
              "       -1.33380e-02,  5.67750e-02,  1.02490e-01, -6.88090e-02,\n",
              "        8.43690e-02, -3.10065e-01,  1.02141e-01,  7.75350e-02,\n",
              "        7.30760e-02,  1.70293e-01,  1.75452e-01, -6.34400e-02,\n",
              "       -1.35530e-01,  5.91510e-02, -2.37317e-01, -3.48400e-03,\n",
              "       -1.74744e-01,  1.17100e-02, -1.69358e-01,  1.93111e-01,\n",
              "        6.51620e-02, -1.68800e-03, -4.09500e-03,  4.04810e-02,\n",
              "        7.89890e-02,  3.04980e-02,  6.58850e-02, -1.85028e-01,\n",
              "       -5.30730e-02,  2.78670e-02,  1.58522e-01, -6.74730e-02,\n",
              "        7.11090e-02, -1.53241e-01, -2.78000e-04,  2.72200e-02,\n",
              "       -5.55410e-02, -1.14370e-02,  2.66986e-01, -8.58900e-02,\n",
              "        2.94660e-02,  6.52130e-02,  1.09075e-01, -1.11196e-01,\n",
              "       -5.21770e-02,  4.16680e-02, -1.42310e-02, -2.10070e-02,\n",
              "        6.34600e-02,  2.95304e-01,  4.77410e-02,  1.56667e-01,\n",
              "       -2.60640e-02, -1.05160e-01, -2.38740e-01,  1.03239e-01,\n",
              "        6.19150e-02, -9.11100e-02,  5.61730e-02,  1.65478e-01,\n",
              "        3.02188e-01, -6.13090e-02, -1.04788e-01,  6.84500e-03,\n",
              "       -6.73840e-02, -5.56240e-02,  8.34690e-02,  4.84860e-02,\n",
              "        2.04896e-01, -3.59160e-02,  2.17440e-02, -1.43469e-01,\n",
              "        7.69030e-02, -2.14700e-01,  2.28290e-02, -4.32890e-02,\n",
              "        1.01490e-01, -2.05650e-02,  1.01706e-01, -4.72350e-02,\n",
              "       -7.35000e-02,  1.04906e-01, -2.10640e-02,  6.03500e-02,\n",
              "       -7.36760e-02, -1.80100e-01,  2.07430e-01, -1.72980e-02,\n",
              "       -1.12444e-01,  6.83430e-02,  3.73490e-02,  1.59640e-02,\n",
              "        1.91593e-01,  1.23727e-01,  4.02420e-02, -9.28090e-02,\n",
              "       -2.27000e-03, -1.18186e-01,  1.52144e-01, -1.06765e-01,\n",
              "        9.42560e-02,  9.16310e-02, -4.62020e-02, -1.90450e-02,\n",
              "       -4.73580e-02, -1.02035e-01,  1.18633e-01, -1.60335e-01,\n",
              "        1.28791e-01, -2.10401e-01,  8.57170e-02,  2.89050e-02,\n",
              "        5.93090e-02, -1.41697e-01, -2.47570e-02,  9.54700e-03,\n",
              "       -4.98800e-02,  2.26029e-01,  1.07696e-01, -3.24990e-02,\n",
              "        6.89630e-02, -2.41557e-01,  3.64990e-02, -3.99500e-02,\n",
              "       -7.69230e-02,  4.26100e-02, -5.31778e-01, -1.04244e-01,\n",
              "       -3.23730e-02, -2.23851e-01, -6.75990e-02,  4.17540e-02,\n",
              "        1.64955e-01,  1.36563e-01,  3.73801e-01, -4.81200e-02,\n",
              "       -2.09910e-02,  7.31410e-02,  6.55180e-02,  1.00147e-01,\n",
              "       -1.36460e-02,  1.42284e-01, -5.14260e-02, -3.22930e-02,\n",
              "        7.11600e-02, -5.02860e-02,  8.99580e-02,  2.99650e-02,\n",
              "       -1.11583e-01,  6.48250e-02,  5.19130e-02,  2.53010e-02,\n",
              "        8.53100e-02,  8.02550e-02, -8.26260e-02, -7.83910e-02,\n",
              "        9.16900e-03, -3.46490e-02, -7.80600e-03,  1.23143e-01,\n",
              "        6.03160e-02, -5.23200e-02,  4.91000e-02,  1.77526e-01,\n",
              "       -1.12930e-01, -7.78680e-02, -9.05890e-02,  6.58400e-03,\n",
              "        7.43040e-02, -3.15950e-02,  8.09380e-02,  2.74110e-02,\n",
              "        9.08430e-02, -1.84616e-01, -6.81900e-03,  1.42920e-02,\n",
              "        1.61837e-01,  4.92000e-02,  3.87000e-02, -8.02370e-02,\n",
              "       -1.73202e-01, -1.59323e-01,  1.64997e-01, -6.27700e-03,\n",
              "       -9.85160e-02,  1.96379e-01, -9.16920e-02, -4.79530e-02,\n",
              "        3.16810e-02,  2.95620e-02,  7.81020e-02, -3.76920e-02,\n",
              "        9.83200e-03,  8.39400e-02, -8.96270e-02, -5.86500e-03,\n",
              "       -1.66240e-01,  1.20850e-02, -4.18520e-02,  6.43990e-02,\n",
              "       -3.02060e-02,  2.40551e-01, -5.03650e-02,  2.08260e-02,\n",
              "        1.08412e-01,  3.44650e-02,  7.11120e-02,  9.48100e-03,\n",
              "       -2.37550e-01, -6.92360e-02, -1.32674e-01,  4.89100e-03,\n",
              "        2.89695e-01, -2.00075e-01,  1.95463e-01,  1.87088e-01,\n",
              "       -1.67430e-02, -2.86308e-01,  6.08500e-03, -7.04400e-02,\n",
              "        7.60110e-02,  9.04110e-02,  3.52390e-02,  4.19360e-02,\n",
              "        1.42461e-01, -1.49700e-02,  4.17880e-02,  4.73040e-02,\n",
              "       -1.86312e-01,  2.14241e-01,  2.15340e-02,  1.33997e-01,\n",
              "        5.28400e-03,  2.28210e-02, -1.98026e-01,  3.83960e-02,\n",
              "       -1.13070e-02,  9.69140e-02, -2.36200e-02,  2.81084e-01,\n",
              "       -1.29797e-01,  1.32496e-01,  1.99220e-02, -1.60195e-01,\n",
              "        1.83550e-02, -8.81530e-02, -3.79020e-02, -1.30320e-01,\n",
              "       -1.15232e-01, -8.66900e-03,  3.91510e-02,  9.93200e-03,\n",
              "        7.54300e-02,  3.49010e-02,  3.17480e-02,  7.60470e-02,\n",
              "       -1.31970e-02, -2.80348e-01, -2.56762e-01,  6.00420e-02,\n",
              "       -1.05869e-01, -3.39130e-01,  1.95708e-01, -1.03843e-01,\n",
              "        9.97720e-02,  2.69316e-01,  5.53000e-02,  3.15568e-01,\n",
              "        1.18024e-01, -1.92804e-01, -1.73930e-01, -5.89820e-02,\n",
              "       -9.14900e-02, -8.39750e-02,  2.54284e-01, -3.71000e-04,\n",
              "        1.78627e-01,  1.25639e-01,  2.87540e-02,  8.29960e-02],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOJKqbvXFOyK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2b40451-1082-412c-d583-e723b9735196"
      },
      "source": [
        "missing_words = 0\n",
        "threshold = 5\n",
        "\n",
        "for word, count in word_counts.items():\n",
        "    if count > threshold:\n",
        "        if word not in embeddings_index:\n",
        "            missing_words += 1\n",
        "            \n",
        "missing_ratio = round(missing_words/len(word_counts),2)*100            \n",
        "print(\"Number of words missing from cc-bn:\", missing_words)\n",
        "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words missing from cc-bn: 6\n",
            "Percent of words that are missing from vocabulary: 0.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpSOV_OPFS5K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "916feda3-9989-4f4a-a313-48dcc58e8eec"
      },
      "source": [
        "vocab_to_int = {} \n",
        "\n",
        "value = 0\n",
        "for word, count in word_counts.items():\n",
        "    if count >= threshold or word in embeddings_index:\n",
        "        vocab_to_int[word] = value\n",
        "        value += 1\n",
        "\n",
        "# Special tokens that will be added to our vocab\n",
        "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
        "\n",
        "# Add codes to vocab\n",
        "for code in codes:\n",
        "    vocab_to_int[code] = len(vocab_to_int)\n",
        "\n",
        "# Dictionary to convert integers to words\n",
        "int_to_vocab = {}\n",
        "for word, value in vocab_to_int.items():\n",
        "    int_to_vocab[value] = word\n",
        "\n",
        "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
        "\n",
        "print(\"Total number of unique words:\", len(word_counts))\n",
        "print(\"Number of words we will use:\", len(vocab_to_int))\n",
        "print(\"Percent of words we will use: {}%\".format(usage_ratio))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of unique words: 6444\n",
            "Number of words we will use: 5516\n",
            "Percent of words we will use: 85.6%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9gKMV2eFX_B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ae6eb42-94f7-4018-85e3-af7c0040f795"
      },
      "source": [
        "embedding_dim = 300\n",
        "nb_words = len(vocab_to_int)\n",
        "\n",
        "# Create matrix with default values of zero\n",
        "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
        "for word, i in vocab_to_int.items():\n",
        "    if word in embeddings_index:\n",
        "        word_embedding_matrix[i] = embeddings_index[word]\n",
        "    else:\n",
        "        # If word not in CN, create a random embedding for it\n",
        "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
        "        embeddings_index[word] = new_embedding\n",
        "        word_embedding_matrix[i] = new_embedding\n",
        "\n",
        "# Check if value matches len(vocab_to_int)\n",
        "print(len(word_embedding_matrix))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzbw-EI7FbGh"
      },
      "source": [
        "def convert_to_ints(text, word_count, unk_count, eos=False):\n",
        "    '''Convert words in text to an integer.\n",
        "       If word is not in vocab_to_int, use UNK's integer.\n",
        "       Total the number of words and UNKs.\n",
        "       Add EOS token to the end of texts'''\n",
        "    ints = []\n",
        "    for sentence in text:\n",
        "        sentence_ints = []\n",
        "        for word in sentence.split():\n",
        "            word_count += 1\n",
        "            if word in vocab_to_int:\n",
        "                sentence_ints.append(vocab_to_int[word])\n",
        "            else:\n",
        "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
        "                unk_count += 1\n",
        "        if eos:\n",
        "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
        "        ints.append(sentence_ints)\n",
        "    return ints, word_count, unk_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZ3JivUIFdtp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "addd81b8-4f53-497f-dd05-0e01a4552049"
      },
      "source": [
        "word_count = 0\n",
        "unk_count = 0\n",
        "\n",
        "int_summaries, word_count, unk_count = convert_to_ints(clean_summaries, word_count, unk_count)\n",
        "int_texts, word_count, unk_count = convert_to_ints(clean_texts, word_count, unk_count, eos=True)\n",
        "\n",
        "unk_percent = round(unk_count/word_count,4)*100\n",
        "\n",
        "print(\"Total number of words in headlines:\", word_count)\n",
        "print(\"Total number of UNKs in headlines:\", unk_count)\n",
        "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of words in headlines: 19679\n",
            "Total number of UNKs in headlines: 1118\n",
            "Percent of words that are UNK: 5.680000000000001%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUrHxTyGFgT5"
      },
      "source": [
        "def create_lengths(text):\n",
        "    '''Create a data frame of the sentence lengths from a text'''\n",
        "    lengths = []\n",
        "    for sentence in text:\n",
        "        lengths.append(len(sentence))\n",
        "    return pd.DataFrame(lengths, columns=['counts'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5FjSDozp4mL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a739a28-7260-436e-818c-a8600ca92f18"
      },
      "source": [
        "lengths_summaries = create_lengths(int_summaries)\n",
        "lengths_texts = create_lengths(int_texts)\n",
        "\n",
        "print(\"Summaries:\")\n",
        "print(lengths_summaries.describe())\n",
        "print()\n",
        "print(\"Texts:\")\n",
        "print(lengths_texts.describe())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summaries:\n",
            "           counts\n",
            "count  212.000000\n",
            "mean    11.132075\n",
            "std      6.749514\n",
            "min      2.000000\n",
            "25%      6.000000\n",
            "50%      9.000000\n",
            "75%     15.000000\n",
            "max     32.000000\n",
            "\n",
            "Texts:\n",
            "           counts\n",
            "count  212.000000\n",
            "mean    82.693396\n",
            "std     41.398713\n",
            "min      2.000000\n",
            "25%     53.000000\n",
            "50%     71.500000\n",
            "75%    106.000000\n",
            "max    284.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HN60lyo-FmFJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c65a1d29-6742-4170-b6b0-19b2566d99a5"
      },
      "source": [
        "print(np.percentile(lengths_texts.counts, 90))\n",
        "print(np.percentile(lengths_texts.counts, 95))\n",
        "print(np.percentile(lengths_texts.counts, 99))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "133.8\n",
            "157.45\n",
            "209.23999999999978\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjEtIi9FFsCy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e16e7496-f6e5-4119-b00b-e2ed209225be"
      },
      "source": [
        "print(np.percentile(lengths_summaries.counts, 90))\n",
        "print(np.percentile(lengths_summaries.counts, 95))\n",
        "print(np.percentile(lengths_summaries.counts, 99))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22.0\n",
            "24.44999999999999\n",
            "29.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhizR3B3Fv5B"
      },
      "source": [
        "def unk_counter(sentence):\n",
        "    '''Counts the number of time UNK appears in a sentence.'''\n",
        "    unk_count = 0\n",
        "    for word in sentence:\n",
        "        if word == vocab_to_int[\"<UNK>\"]:\n",
        "            unk_count += 1\n",
        "    return unk_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZc7r7kgFzJK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b90d6a1-229f-485b-d550-72c8792a8fdf"
      },
      "source": [
        "sorted_summaries = []\n",
        "sorted_texts = []\n",
        "max_text_length = 83\n",
        "max_summary_length =13\n",
        "min_length = 2\n",
        "unk_text_limit = 1\n",
        "unk_summary_limit = 0\n",
        "\n",
        "for length in range(min(lengths_texts.counts), max_text_length): \n",
        "    for count, words in enumerate(int_summaries):\n",
        "        if (len(int_summaries[count]) >= min_length and\n",
        "            len(int_summaries[count]) <= max_summary_length and\n",
        "            len(int_texts[count]) >= min_length and\n",
        "            unk_counter(int_summaries[count]) <= unk_summary_limit and\n",
        "            unk_counter(int_texts[count]) <= unk_text_limit and\n",
        "            length == len(int_texts[count])\n",
        "           ):\n",
        "            sorted_summaries.append(int_summaries[count])\n",
        "            sorted_texts.append(int_texts[count])\n",
        "        \n",
        "# Compare lengths to ensure they match\n",
        "print(len(sorted_summaries))\n",
        "print(len(sorted_texts))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24\n",
            "24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4rnIJSuF2Q7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc9adbd4-958b-416b-ca82-c58a70a2c5e3"
      },
      "source": [
        "!pip install tensorflow==1.15\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from tensorflow.python.layers.core import Dense\n",
        "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
        "print('TensorFlow Version: {}'.format(tf.__version__))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==1.15\n",
            "  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3 MB 20 kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.44.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.0.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.21.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.17.3)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 34.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.37.1)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 25.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.14.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (3.1.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.7.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15) (1.5.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=e85a7ca9a02ead9bf76103017f446bc805215faef2dfaf9352a5521045aebedf\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.0\n",
            "    Uninstalling tensorflow-2.8.0:\n",
            "      Successfully uninstalled tensorflow-2.8.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.16.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n",
            "TensorFlow Version: 1.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bx3cELWbqRKZ"
      },
      "source": [
        "def model_inputs():\n",
        "    '''Create palceholders for inputs to the model'''\n",
        "    \n",
        "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
        "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
        "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
        "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
        "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
        "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
        "\n",
        "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HklvwM4VGbEE"
      },
      "source": [
        "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
        "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
        "    \n",
        "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
        "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
        "\n",
        "    return dec_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1Sn9tLcGgqx"
      },
      "source": [
        "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
        "    '''Create the encoding layer'''\n",
        "    \n",
        "    for layer in range(num_layers):\n",
        "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
        "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
        "                                                    input_keep_prob = keep_prob)\n",
        "\n",
        "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
        "                                                    input_keep_prob = keep_prob)\n",
        "\n",
        "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
        "                                                                    cell_bw, \n",
        "                                                                    rnn_inputs,\n",
        "                                                                    sequence_length,\n",
        "                                                                    dtype=tf.float32)\n",
        "    # Join outputs since we are using a bidirectional RNN\n",
        "    enc_output = tf.concat(enc_output,2)\n",
        "    \n",
        "    return enc_output, enc_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3r5mNR9GkPL"
      },
      "source": [
        "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, initial_state, output_layer, \n",
        "                            vocab_size, max_summary_length):\n",
        "    '''Create the training logits'''\n",
        "    \n",
        "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
        "                                                        sequence_length=summary_length,\n",
        "                                                        time_major=False)\n",
        "\n",
        "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                       training_helper,\n",
        "                                                       initial_state,\n",
        "                                                       output_layer) \n",
        "\n",
        "    training_logits, _ , _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
        "                                                           output_time_major=False,\n",
        "                                                           impute_finished=True,\n",
        "                                                           maximum_iterations=max_summary_length)\n",
        "    return training_decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lb5Jjg_GnNR"
      },
      "source": [
        "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n",
        "                             max_summary_length, batch_size):\n",
        "    '''Create the inference logits'''\n",
        "    \n",
        "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
        "    \n",
        "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
        "                                                                start_tokens,\n",
        "                                                                end_token)\n",
        "                \n",
        "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                        inference_helper,\n",
        "                                                        initial_state,\n",
        "                                                        output_layer)\n",
        "                \n",
        "    inference_logits, _ , _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
        "                                                            output_time_major=False,\n",
        "                                                            impute_finished=True,\n",
        "                                                            maximum_iterations=max_summary_length)\n",
        "    \n",
        "    return inference_decoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePiiL1XEGqTp"
      },
      "source": [
        "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length, \n",
        "                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
        "    '''Create the decoding cell and attention for the training and inference decoding layers'''\n",
        "    \n",
        "    for layer in range(num_layers):\n",
        "        with tf.variable_scope('decoder_{}'.format(layer)):\n",
        "            lstm = tf.contrib.rnn.LSTMCell(rnn_size,\n",
        "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
        "            dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, \n",
        "                                                     input_keep_prob = keep_prob)\n",
        "    \n",
        "    output_layer = Dense(vocab_size,\n",
        "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
        "    \n",
        "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
        "                                                  enc_output,\n",
        "                                                  text_length,\n",
        "                                                  normalize=False,\n",
        "                                                  name='BahdanauAttention')\n",
        "\n",
        "    dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,\n",
        "                                                          attn_mech,\n",
        "                                                          rnn_size)\n",
        "    #initial_state = tf.contrib.seq2seq.AttentionWrapperState(enc_state[0],\n",
        "    #                                                                _zero_state_tensors(rnn_size, \n",
        "    #                                                                                    batch_size, \n",
        "    #                                                                                    tf.float32)) \n",
        "    initial_state = dec_cell.zero_state(batch_size=batch_size,dtype=tf.float32).clone(cell_state=enc_state[0])\n",
        "\n",
        "    with tf.variable_scope(\"decode\"):\n",
        "        training_decoder = training_decoding_layer(dec_embed_input, \n",
        "                                                  summary_length, \n",
        "                                                  dec_cell, \n",
        "                                                  initial_state,\n",
        "                                                  output_layer,\n",
        "                                                  vocab_size, \n",
        "                                                  max_summary_length)\n",
        "        \n",
        "        training_logits,_ ,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
        "                                  output_time_major=False,\n",
        "                                  impute_finished=True,\n",
        "                                  maximum_iterations=max_summary_length)\n",
        "    with tf.variable_scope(\"decode\", reuse=True):\n",
        "        inference_decoder = inference_decoding_layer(embeddings,  \n",
        "                                                    vocab_to_int['<GO>'], \n",
        "                                                    vocab_to_int['<EOS>'],\n",
        "                                                    dec_cell, \n",
        "                                                    initial_state, \n",
        "                                                    output_layer,\n",
        "                                                    max_summary_length,\n",
        "                                                    batch_size)\n",
        "        \n",
        "        inference_logits,_ ,_ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
        "                                  output_time_major=False,\n",
        "                                  impute_finished=True,\n",
        "                                  maximum_iterations=max_summary_length)\n",
        "\n",
        "    return training_logits, inference_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TialSOTrGuJ0"
      },
      "source": [
        "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
        "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
        "    '''Use the previous functions to create the training and inference logits'''\n",
        "    \n",
        "    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n",
        "    embeddings = word_embedding_matrix\n",
        "    \n",
        "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
        "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
        "    \n",
        "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size)\n",
        "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
        "    \n",
        "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
        "                                                        embeddings,\n",
        "                                                        enc_output,\n",
        "                                                        enc_state, \n",
        "                                                        vocab_size, \n",
        "                                                        text_length, \n",
        "                                                        summary_length, \n",
        "                                                        max_summary_length,\n",
        "                                                        rnn_size, \n",
        "                                                        vocab_to_int, \n",
        "                                                        keep_prob, \n",
        "                                                        batch_size,\n",
        "                                                        num_layers)\n",
        "    \n",
        "    return training_logits, inference_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdaOm5tDG93a"
      },
      "source": [
        "def pad_sentence_batch(sentence_batch):\n",
        "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
        "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
        "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-WbB51JHBC6"
      },
      "source": [
        "def get_batches(summaries, texts, batch_size):\n",
        "    \"\"\"Batch summaries, texts, and the lengths of their sentences together\"\"\"\n",
        "    for batch_i in range(0, len(texts)//batch_size):\n",
        "        start_i = batch_i * batch_size\n",
        "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
        "        texts_batch = texts[start_i:start_i + batch_size]\n",
        "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
        "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
        "        \n",
        "        # Need the lengths for the _lengths parameters\n",
        "        pad_summaries_lengths = []\n",
        "        for summary in pad_summaries_batch:\n",
        "            pad_summaries_lengths.append(len(summary))\n",
        "        \n",
        "        pad_texts_lengths = []\n",
        "        for text in pad_texts_batch:\n",
        "            pad_texts_lengths.append(len(text))\n",
        "        \n",
        "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gC78l5mdHFbA"
      },
      "source": [
        "epochs = 40\n",
        "batch_size = 2\n",
        "rnn_size = 128\n",
        "num_layers = 3\n",
        "learning_rate = 0.001\n",
        "keep_probability = 0.70"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FawKgFoAHKYh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a14baccc-bd23-4720-874f-f941a08d1ab3"
      },
      "source": [
        "train_graph = tf.Graph()\n",
        "# Set the graph to default to ensure that it is ready for training\n",
        "with train_graph.as_default():\n",
        "    \n",
        "    # Load the model inputs    \n",
        "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
        "\n",
        "    # Create the training and inference logits\n",
        "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
        "                                                      targets, \n",
        "                                                      keep_prob,   \n",
        "                                                      text_length,\n",
        "                                                      summary_length,\n",
        "                                                      max_summary_length,\n",
        "                                                      len(vocab_to_int)+1,\n",
        "                                                      rnn_size, \n",
        "                                                      num_layers, \n",
        "                                                      vocab_to_int,\n",
        "                                                      batch_size)\n",
        "    \n",
        "    # Create tensors for the training logits and inference logits\n",
        "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
        "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
        "    \n",
        "    # Create the weights for sequence_loss\n",
        "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
        "\n",
        "    with tf.name_scope(\"optimization\"):\n",
        "        # Loss function\n",
        "        cost = tf.contrib.seq2seq.sequence_loss(\n",
        "            training_logits,\n",
        "            targets,\n",
        "            masks)\n",
        "\n",
        "        # Optimizer\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "\n",
        "        # Gradient Clipping\n",
        "        gradients = optimizer.compute_gradients(cost)\n",
        "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
        "        train_op = optimizer.apply_gradients(capped_gradients)\n",
        "print(\"Graph is built.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-28-9390cb26606a>:7: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-28-9390cb26606a>:20: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Graph is built.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VP5viJK1HODZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e42ccef-62e7-4a5e-82ce-3964253f6676"
      },
      "source": [
        "start = 5\n",
        "end = start + 5\n",
        "sorted_summaries_short = sorted_summaries[start:end]\n",
        "sorted_texts_short = sorted_texts[start:end]\n",
        "print(\"The shortest text length:\", len(sorted_texts_short[0]))\n",
        "print(\"The longest text length:\",len(sorted_texts_short[-1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shortest text length: 39\n",
            "The longest text length: 44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECzcxD4ht2m4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bc790ce-67b2-4c0a-b725-5d9226ead8ef"
      },
      "source": [
        "# Train the Model\n",
        "learning_rate_decay = 0.90\n",
        "min_learning_rate = 0.001\n",
        "display_step = 2# Check training loss after every 20 batches\n",
        "stop_early = 0 \n",
        "stop = 3 #3 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
        "per_epoch = 1 # Make 3 update checks per epoch\n",
        "update_check = (len(sorted_texts_short)//batch_size//per_epoch)-1\n",
        "\n",
        "update_loss = 0 \n",
        "batch_loss = 0\n",
        "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
        "\n",
        "  \n",
        "tf.reset_default_graph()\n",
        "checkpoint = \"./model1.ckpt\"  #300k sentence\n",
        "with tf.Session(graph=train_graph) as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    # If we want to continue training a previous session\n",
        "    # loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
        "    # loader.restore(sess, checkpoint)\n",
        "    #sess.run(tf.local_variables_initializer())\n",
        "\n",
        "    for epoch_i in range(1, epochs+1):\n",
        "        update_loss = 0\n",
        "        batch_loss = 0\n",
        "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
        "                get_batches(sorted_summaries_short, sorted_texts_short, batch_size)):\n",
        "            start_time = time.time()\n",
        "            _, loss = sess.run(\n",
        "                [train_op, cost],\n",
        "                {input_data: texts_batch,\n",
        "                 targets: summaries_batch,\n",
        "                 lr: learning_rate,\n",
        "                 summary_length: summaries_lengths,\n",
        "                 text_length: texts_lengths,\n",
        "                 keep_prob: keep_probability})\n",
        "\n",
        "            batch_loss += loss\n",
        "            update_loss += loss\n",
        "            end_time = time.time()\n",
        "            batch_time = end_time - start_time\n",
        "\n",
        "            if batch_i % display_step == 0 and batch_i > 0:\n",
        "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
        "                      .format(epoch_i,\n",
        "                              epochs, \n",
        "                              batch_i, \n",
        "                              len(sorted_texts_short) // batch_size, \n",
        "                              batch_loss / display_step, \n",
        "                              batch_time*display_step))\n",
        "                batch_loss = 0\n",
        "                \n",
        "                #saver = tf.train.Saver() \n",
        "                #saver.save(sess, checkpoint)\n",
        "                \n",
        "            if batch_i % update_check == 0 and batch_i > 0:\n",
        "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
        "                summary_update_loss.append(update_loss)\n",
        "                \n",
        "              \n",
        "                  \n",
        "                # If the update loss is at a new minimum, save the model\n",
        "                if update_loss <= min(summary_update_loss):\n",
        "                    print('New Record!') \n",
        "                    stop_early = 0\n",
        "                    saver = tf.train.Saver() \n",
        "                    saver.save(sess, checkpoint)\n",
        "\n",
        "                else:\n",
        "                    print(\"No Improvement.\")\n",
        "                    stop_early += 1\n",
        "                    if stop_early == stop:\n",
        "                        break\n",
        "                update_loss = 0\n",
        "            \n",
        "                    \n",
        "        # Reduce learning rate, but not below its minimum value\n",
        "        learning_rate *= learning_rate_decay\n",
        "        if learning_rate < min_learning_rate:\n",
        "            learning_rate = min_learning_rate\n",
        "        \n",
        "        if stop_early == stop:\n",
        "            print(\"Stopping Training.\")\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average loss for this update: 17.137\n",
            "New Record!\n",
            "Average loss for this update: 15.794\n",
            "New Record!\n",
            "Average loss for this update: 14.218\n",
            "New Record!\n",
            "Average loss for this update: 12.187\n",
            "New Record!\n",
            "Average loss for this update: 11.028\n",
            "New Record!\n",
            "Average loss for this update: 9.997\n",
            "New Record!\n",
            "Average loss for this update: 8.77\n",
            "New Record!\n",
            "Average loss for this update: 7.625\n",
            "New Record!\n",
            "Average loss for this update: 6.423\n",
            "New Record!\n",
            "Average loss for this update: 5.476\n",
            "New Record!\n",
            "Average loss for this update: 4.547\n",
            "New Record!\n",
            "Average loss for this update: 3.95\n",
            "New Record!\n",
            "Average loss for this update: 3.472\n",
            "New Record!\n",
            "Average loss for this update: 3.124\n",
            "New Record!\n",
            "Average loss for this update: 2.783\n",
            "New Record!\n",
            "Average loss for this update: 2.656\n",
            "New Record!\n",
            "Average loss for this update: 2.392\n",
            "New Record!\n",
            "Average loss for this update: 2.253\n",
            "New Record!\n",
            "Average loss for this update: 2.2\n",
            "New Record!\n",
            "Average loss for this update: 2.019\n",
            "New Record!\n",
            "Average loss for this update: 1.953\n",
            "New Record!\n",
            "Average loss for this update: 1.849\n",
            "New Record!\n",
            "Average loss for this update: 1.783\n",
            "New Record!\n",
            "Average loss for this update: 1.707\n",
            "New Record!\n",
            "Average loss for this update: 1.646\n",
            "New Record!\n",
            "Average loss for this update: 1.604\n",
            "New Record!\n",
            "Average loss for this update: 1.552\n",
            "New Record!\n",
            "Average loss for this update: 1.505\n",
            "New Record!\n",
            "Average loss for this update: 1.429\n",
            "New Record!\n",
            "Average loss for this update: 1.446\n",
            "No Improvement.\n",
            "Average loss for this update: 1.36\n",
            "New Record!\n",
            "Average loss for this update: 1.27\n",
            "New Record!\n",
            "Average loss for this update: 1.217\n",
            "New Record!\n",
            "Average loss for this update: 1.172\n",
            "New Record!\n",
            "Average loss for this update: 1.204\n",
            "No Improvement.\n",
            "Average loss for this update: 1.162\n",
            "New Record!\n",
            "Average loss for this update: 1.117\n",
            "New Record!\n",
            "Average loss for this update: 1.098\n",
            "New Record!\n",
            "Average loss for this update: 1.04\n",
            "New Record!\n",
            "Average loss for this update: 1.036\n",
            "New Record!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pARBo94YIKZK"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}